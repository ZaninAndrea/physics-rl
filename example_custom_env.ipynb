{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dojo import Dojo\n",
    "from tf_agents.environments import suite_gym, tf_py_environment, py_environment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action == 1:\n",
    "      self._episode_ended = True\n",
    "    elif action == 0:\n",
    "      new_card = np.random.randint(1, 11)\n",
    "      self._state += new_card\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = suite_gym.load('CartPole-v1')\n",
    "env = CardGameEnv()\n",
    "env = tf_py_environment.TFPyEnvironment(env)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = QNetwork(env.observation_spec(), env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 24.986818313598633 return = -12.5\n",
      "step = 400: loss = 4.448462963104248 return = -11.100000381469727\n",
      "step = 600: loss = 4.064504623413086 return = -6.300000190734863\n",
      "step = 800: loss = 3.6110057830810547 return = -6.699999809265137\n",
      "step = 1000: loss = 3.1352672576904297 return = -5.099999904632568\n",
      "step = 1200: loss = 3.2211601734161377 return = -6.300000190734863\n",
      "step = 1400: loss = 3.2564537525177 return = -5.0\n",
      "step = 1600: loss = 2.854937791824341 return = -6.0\n",
      "step = 1800: loss = 2.9084932804107666 return = -4.800000190734863\n",
      "step = 2000: loss = 3.150632858276367 return = -4.699999809265137\n",
      "step = 2200: loss = 3.1324493885040283 return = -6.400000095367432\n",
      "step = 2400: loss = 2.9978902339935303 return = -5.099999904632568\n",
      "step = 2600: loss = 3.132115125656128 return = -4.599999904632568\n",
      "step = 2800: loss = 3.04097843170166 return = -4.0\n",
      "step = 3000: loss = 3.404898166656494 return = -5.900000095367432\n",
      "step = 3200: loss = 2.818737506866455 return = -6.099999904632568\n",
      "step = 3400: loss = 2.9395394325256348 return = -3.0\n",
      "step = 3600: loss = 2.993468999862671 return = -6.099999904632568\n",
      "step = 3800: loss = 2.821033239364624 return = -5.199999809265137\n",
      "step = 4000: loss = 3.21162748336792 return = -3.0\n",
      "step = 4200: loss = 2.9384143352508545 return = -3.0999999046325684\n",
      "step = 4400: loss = 3.4076733589172363 return = -4.900000095367432\n",
      "step = 4600: loss = 3.020343065261841 return = -4.0\n",
      "step = 4800: loss = 3.494884729385376 return = -5.199999809265137\n",
      "step = 5000: loss = 3.4343819618225098 return = -6.400000095367432\n",
      "step = 5200: loss = 3.4826743602752686 return = -4.0\n",
      "step = 5400: loss = 3.085726261138916 return = -6.300000190734863\n",
      "step = 5600: loss = 3.3071987628936768 return = -7.5\n",
      "step = 5800: loss = 2.959648370742798 return = -4.599999904632568\n",
      "step = 6000: loss = 3.4746804237365723 return = -4.900000095367432\n",
      "step = 6200: loss = 3.2920563220977783 return = -5.400000095367432\n",
      "step = 6400: loss = 3.683412790298462 return = -2.9000000953674316\n",
      "step = 6600: loss = 3.614499568939209 return = -5.300000190734863\n",
      "step = 6800: loss = 3.4492270946502686 return = -4.400000095367432\n",
      "step = 7000: loss = 3.6662750244140625 return = -5.599999904632568\n",
      "step = 7200: loss = 3.468614101409912 return = -4.800000190734863\n",
      "step = 7400: loss = 3.4635837078094482 return = -4.699999809265137\n",
      "step = 7600: loss = 3.3092234134674072 return = -5.599999904632568\n",
      "step = 7800: loss = 3.622603416442871 return = -6.300000190734863\n",
      "step = 8000: loss = 3.160712957382202 return = -5.0\n",
      "step = 8200: loss = 3.386122226715088 return = -4.0\n",
      "step = 8400: loss = 2.9208359718322754 return = -4.0\n",
      "step = 8600: loss = 3.444237470626831 return = -5.300000190734863\n",
      "step = 8800: loss = 3.5218355655670166 return = -6.300000190734863\n",
      "step = 9000: loss = 3.4351186752319336 return = -8.699999809265137\n",
      "step = 9200: loss = 3.3742592334747314 return = -4.5\n",
      "step = 9400: loss = 3.3907485008239746 return = -7.900000095367432\n",
      "step = 9600: loss = 3.052837610244751 return = -2.799999952316284\n",
      "step = 9800: loss = 3.693941116333008 return = -4.599999904632568\n",
      "step = 10000: loss = 3.4249320030212402 return = -5.199999809265137\n"
     ]
    }
   ],
   "source": [
    "dojo = Dojo(q_net, env)\n",
    "dojo.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
